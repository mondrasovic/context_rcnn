{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "from src.models import AttentionEmbMapper\n",
    "from src.models import ContextAttention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 512]         131,072\n",
      "              ReLU-2                  [-1, 512]               0\n",
      "            Linear-3                  [-1, 256]         131,072\n",
      "AttentionEmbMapper-4                  [-1, 256]               0\n",
      "            Linear-5                  [-1, 512]         131,072\n",
      "              ReLU-6                  [-1, 512]               0\n",
      "            Linear-7                  [-1, 256]         131,072\n",
      "AttentionEmbMapper-8                  [-1, 256]               0\n",
      "            Linear-9                  [-1, 512]         131,072\n",
      "             ReLU-10                  [-1, 512]               0\n",
      "           Linear-11                  [-1, 256]         131,072\n",
      "AttentionEmbMapper-12                  [-1, 256]               0\n",
      "           Linear-13                  [-1, 512]         131,072\n",
      "             ReLU-14                  [-1, 512]               0\n",
      "           Linear-15                  [-1, 256]         131,072\n",
      "AttentionEmbMapper-16                  [-1, 256]               0\n",
      "================================================================\n",
      "Total params: 1,048,576\n",
      "Trainable params: 1,048,576\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 600.25\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 4.00\n",
      "Estimated Total Size (MB): 604.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_proposals = 512\n",
    "batch_size = 4\n",
    "n_channels = 256\n",
    "feature_size = 7\n",
    "emb_dim = 64\n",
    "\n",
    "features = torch.rand(\n",
    "    (n_proposals * batch_size, n_channels, feature_size, feature_size)\n",
    ")\n",
    "input_size = (n_channels, feature_size, feature_size)\n",
    "\n",
    "model = ContextAttention(n_channels).cuda()\n",
    "summary(model, [input_size, input_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6771, -0.1115, -0.3210,  0.0090,  0.1004, -0.2192, -0.0292,  0.4988,\n",
       "         -0.1709,  0.1246, -0.2518,  0.3792,  0.0070,  0.1795, -0.2560,  0.6847,\n",
       "          0.1022,  0.0686, -0.1392,  0.0927,  0.1786, -0.5427, -0.5680, -0.6615,\n",
       "         -0.7284, -0.1208, -0.2196,  0.2218,  0.9489,  0.1607, -0.0897,  0.1481,\n",
       "         -0.5095, -0.5655, -0.0495,  0.6000, -0.0437, -0.7645,  0.0268, -0.1941,\n",
       "          0.1351,  0.1996,  0.0038, -0.0291, -0.6222,  0.1916, -0.5216,  0.3000,\n",
       "         -0.5842, -0.5233, -0.0615, -0.7544, -0.5112,  0.7046, -0.1274,  0.0274,\n",
       "         -0.5609, -0.2589,  0.2220,  0.3193, -0.5961, -0.4629, -0.5731,  0.1050]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dim = 10\n",
    "out_dim = 64\n",
    "\n",
    "class TmpModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        print(f\"Training: {self.training}\")\n",
    "        return x\n",
    "\n",
    "data = torch.rand((1, in_dim))\n",
    "module = TmpModule()\n",
    "module(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f91a5d6e4747d83f2ca8abaaaf7aacd52eac54e528eee770e3eb228d51a3694d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
